{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JobPosting_Tensorflow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK3ytYXu4u4D",
        "colab_type": "text"
      },
      "source": [
        "## **[Real or Fake] Job Posting using Tensorflow**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttDsTWwSM_0w",
        "colab_type": "text"
      },
      "source": [
        "The idea of this project is to convert our previous implementation from scratch to a tensorflow 1 implementation. Here, we deal with the same Fake job posting dataset and implement firstly a single layer neural network and then extend it to a multi layer implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f23trmBa3ffQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "7167bcfa-69de-454d-c8e6-7c9bfe839be4"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math\n",
        "import seaborn as sns\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from tensorflow.python.framework import ops"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_-T1ZNt32-T",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "86355a5b-1ec5-482d-ddac-e6d8daa4958e"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6ce731eb-73ac-4c28-92c7-da6bb5bba628\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6ce731eb-73ac-4c28-92c7-da6bb5bba628\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving fake_job_postings.csv to fake_job_postings.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KUUfgZr36lD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('fake_job_postings.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5TDcwQR3-kE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['country'] = df['location'].str.split(',', expand=True)[0]\n",
        "df['state'] = df['location'].str.split(',', expand=True)[2]\n",
        "df['city'] = df['location'].str.split(',', expand=True)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8UH_fMM4DSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['has_company_profile'] = df['company_profile'].isnull().astype(int)\n",
        "df['has_salary_range'] = df['salary_range'].isnull().astype(int)\n",
        "df['has_required_experience'] = df['required_experience'].isnull().astype(int)\n",
        "df['has_required_education'] = df['required_education'].isnull().astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeOjD-6MXiph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_sampled_0 = df[df['fraudulent']==0].sample(n = 866)\n",
        "df_sampled = pd.concat([df_sampled_0, df[df['fraudulent']==1]]).sample(frac=1).reset_index(drop=True)\n",
        "df_sampled.drop(['job_id', 'location', 'salary_range', 'company_profile', 'description', 'requirements', 'benefits'], axis=1, inplace=True)\n",
        "df_sampled.fillna('<None>', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOP7JeLb4Ok1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        " \n",
        "\n",
        "columns_to_encode =  ['title', 'department', 'employment_type', 'required_experience', 'required_education', 'industry', 'function', 'country', 'state', 'city']\n",
        "ohe    = OneHotEncoder(sparse=False)\n",
        "encoded_columns =    ohe.fit_transform(df_sampled[columns_to_encode])\n",
        "encoded_df = pd.DataFrame(encoded_columns)\n",
        "ohe.get_feature_names()\n",
        "encoded_df.columns = ohe.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo4vkL7Z4SWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_sampled_mod = df_sampled.drop(labels = columns_to_encode, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EzInC_q4VS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_final = pd.concat([df_sampled_mod, encoded_df], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK3rJ-Gx4W1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df_final.loc[:, df_final.columns != 'fraudulent']\n",
        "y = df_final['fraudulent']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpmfi3onrFVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHC35GW3rGmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCh_pSQOjkSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_flat = X_train.values.T\n",
        "X_test_flat = X_test.values.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-wNM2qeTLho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_flat = y_train.values.reshape(1, -1)\n",
        "y_test_flat = y_test.values.reshape(1, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy6fA7ocwBqa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "932496c6-c17e-485b-b3e7-240180ab13b0"
      },
      "source": [
        "X_train_flat.shape, y_train_flat.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2494, 1212), (1, 1212))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6_-tHnm4o8R",
        "colab_type": "text"
      },
      "source": [
        "### **Linear Regression using Tensorflow**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD07KzgU4tGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.placeholder(\"float\", shape = (X_train_flat.shape[0], None), name = 'x')\n",
        "initializer = tf.initializers.glorot_uniform()\n",
        "W = tf.Variable(initializer(shape = [1, X_train_flat.shape[0]]), name = 'W', dtype = \"float32\")\n",
        "b = tf.zeros([1], name = 'b')\n",
        "y = tf.placeholder(\"float\", shape = [1, None])\n",
        "Z = tf.matmul(W, x) + b\n",
        "activation = tf.math.sigmoid(Z)\n",
        "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Z, labels = y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
        "prediction = tf.round(activation)\n",
        "correct = tf.cast(tf.equal(prediction, y), dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(correct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8RsIpZSDFpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "b35c3768-dff4-4907-d038-01f5fa8c73ad"
      },
      "source": [
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "init = tf.initialize_all_variables()\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  iteration = []\n",
        "  x_entropy = []\n",
        "  for i in range(3000):\n",
        "    avg_cost = 0\n",
        "    sess.run(optimizer, feed_dict = {x: X_train_flat, y:y_train_flat})\n",
        "    avg_cost = sess.run(cost, feed_dict = {x: X_train_flat, y:y_train_flat})\n",
        "    temp_train_acc = sess.run(accuracy, feed_dict={x: X_train_flat, y: y_train_flat})\n",
        "    temp_test_acc = sess.run(accuracy, feed_dict={x: X_test_flat, y: y_test_flat})\n",
        "    if i % 500 == 0:\n",
        "      print(\"Iteration:\", i, \"cost=\", avg_cost, \"training accuracy=\", temp_train_acc, \"test accuracy = \", temp_test_acc)\n",
        "    x_entropy.append(avg_cost)\n",
        "    train_acc.append(temp_train_acc)\n",
        "    test_acc.append(temp_test_acc)\n",
        "    iteration.append(i)\n",
        "    \n",
        "  print (\"Training phase finished\")\n",
        "  plt.plot(iteration, x_entropy, 'o', label='Logistic Regression Training phase')\n",
        "  plt.ylabel('cost')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:235: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "Iteration: 0 cost= 0.68317145 training accuracy= 0.5750825 test accuracy =  0.58076924\n",
            "Iteration: 500 cost= 0.3722579 training accuracy= 0.8547855 test accuracy =  0.8403846\n",
            "Iteration: 1000 cost= 0.3278583 training accuracy= 0.8828383 test accuracy =  0.84615386\n",
            "Iteration: 1500 cost= 0.2997002 training accuracy= 0.8894389 test accuracy =  0.85384613\n",
            "Iteration: 2000 cost= 0.27835396 training accuracy= 0.8968647 test accuracy =  0.8576923\n",
            "Iteration: 2500 cost= 0.26101398 training accuracy= 0.910066 test accuracy =  0.86346155\n",
            "Training phase finished\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtiDLsMsNcu4",
        "colab_type": "text"
      },
      "source": [
        "Here, as you can see, we used Adam optimizer and got a training accuracy of 91% and test accuracy of 86% which is not bad considering only one sigmoid layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb-x63rbGxpY",
        "colab_type": "text"
      },
      "source": [
        "### **Deep Learning using Tensorflow**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjrMrGxXNozd",
        "colab_type": "text"
      },
      "source": [
        "Now for multi layer implementation, in the following example we consider three layers. The first layer is the input layer, and all the layers except for the final layer has relu as activation. The final layer is sigmoid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz-M1Ye8I9qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers_dims = [X_train_flat.shape[0], 5, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yngHUQ35JEnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_deep():\n",
        "  parameters = {}\n",
        "  initializer1 = tf.initializers.glorot_uniform()\n",
        "  initializer2 = tf.compat.v1.zeros_initializer()\n",
        "  \n",
        "  L = len(layers_dims)\n",
        "  for i in range(L-1):\n",
        "    W = tf.Variable(initializer1(shape = [layers_dims[i+1], layers_dims[i]]), name = 'W' + str(i+1), dtype = \"float32\")\n",
        "    b = tf.Variable(initializer2(shape = [layers_dims[i+1], 1]), name = 'b' + str(i+1), dtype = \"float32\")\n",
        "    parameters['W' + str(i+1)] = W\n",
        "    parameters['b' + str(i+1)] = b\n",
        "  return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYnp2BDTFPRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagation(A, parameters):\n",
        "  L = len(layers_dims)\n",
        "  for i in range(L-2):\n",
        "    W = parameters['W' + str(i+1)]\n",
        "    b = parameters['b' + str(i+1)]\n",
        "    A = tf.nn.relu(tf.matmul(W, A) + b)\n",
        "\n",
        "  WL = parameters['W' + str(L-1)]\n",
        "  bL = parameters['b' + str(L-1)]\n",
        "  ZL = tf.matmul(WL, A) + bL\n",
        "  return ZL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtUjNhFsYHrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001):\n",
        "  ops.reset_default_graph()\n",
        "  X = tf.placeholder(\"float\", shape = (X_train_flat.shape[0], None), name = 'X')\n",
        "  y = tf.placeholder(\"float\", shape = [1, None], name = 'y')\n",
        "  parameters = initialize_deep()\n",
        "  ZL = forward_propagation(X, parameters)\n",
        "  cost = cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = ZL, labels = y))\n",
        "  activation = tf.math.sigmoid(ZL)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "  prediction = tf.round(activation)\n",
        "  correct = tf.cast(tf.equal(prediction, y), dtype=tf.float32)\n",
        "  accuracy = tf.reduce_mean(correct)\n",
        "\n",
        "  init = tf.global_variables_initializer()\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    x_entropy = []\n",
        "    for i in range(3000):\n",
        "      avg_cost = 0\n",
        "      _, avg_cost = sess.run([optimizer, cost], feed_dict = {X: X_train_flat, y:y_train_flat})\n",
        "      x_entropy.append(avg_cost)\n",
        "      temp_train_acc = sess.run(accuracy, feed_dict={X: X_train_flat, y: y_train_flat})\n",
        "      temp_test_acc = sess.run(accuracy, feed_dict={X: X_test_flat, y: y_test_flat})\n",
        "      if i % 100 == 0:\n",
        "        print(\"Iteration:\", i, \"cost=\", avg_cost, \"training accuracy=\", temp_train_acc, \"test accuracy =\", temp_test_acc)\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCT1V02xaMJy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "27d77aca-32eb-4d27-cacf-6ff606f9de0a"
      },
      "source": [
        "parameters = model(X_train_flat, y_train_flat, X_test_flat, y_test_flat, learning_rate = 0.0001)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0 cost= 0.69110125 training accuracy= 0.56848186 test accuracy = 0.5692308\n",
            "Iteration: 100 cost= 0.6577348 training accuracy= 0.8506601 test accuracy = 0.7865385\n",
            "Iteration: 200 cost= 0.61750746 training accuracy= 0.9232673 test accuracy = 0.84807694\n",
            "Iteration: 300 cost= 0.57833344 training accuracy= 0.93976897 test accuracy = 0.8576923\n",
            "Iteration: 400 cost= 0.5405294 training accuracy= 0.9447195 test accuracy = 0.86346155\n",
            "Iteration: 500 cost= 0.5040674 training accuracy= 0.95049506 test accuracy = 0.86346155\n",
            "Iteration: 600 cost= 0.46917492 training accuracy= 0.9554455 test accuracy = 0.86730766\n",
            "Iteration: 700 cost= 0.43613964 training accuracy= 0.9612211 test accuracy = 0.86730766\n",
            "Iteration: 800 cost= 0.40514606 training accuracy= 0.9669967 test accuracy = 0.86346155\n",
            "Iteration: 900 cost= 0.3762211 training accuracy= 0.9686469 test accuracy = 0.86538464\n",
            "Iteration: 1000 cost= 0.34933287 training accuracy= 0.9711221 test accuracy = 0.86538464\n",
            "Iteration: 1100 cost= 0.32447585 training accuracy= 0.9752475 test accuracy = 0.86153847\n",
            "Iteration: 1200 cost= 0.30155286 training accuracy= 0.9768977 test accuracy = 0.86730766\n",
            "Iteration: 1300 cost= 0.28043643 training accuracy= 0.97772276 test accuracy = 0.86923075\n",
            "Iteration: 1400 cost= 0.26099798 training accuracy= 0.9793729 test accuracy = 0.87115383\n",
            "Iteration: 1500 cost= 0.24313512 training accuracy= 0.9818482 test accuracy = 0.86923075\n",
            "Iteration: 1600 cost= 0.22671725 training accuracy= 0.98432344 test accuracy = 0.87115383\n",
            "Iteration: 1700 cost= 0.21161976 training accuracy= 0.9851485 test accuracy = 0.8730769\n",
            "Iteration: 1800 cost= 0.19772765 training accuracy= 0.9851485 test accuracy = 0.875\n",
            "Iteration: 1900 cost= 0.18493408 training accuracy= 0.9867987 test accuracy = 0.87884617\n",
            "Iteration: 2000 cost= 0.17313944 training accuracy= 0.98762375 test accuracy = 0.88076925\n",
            "Iteration: 2100 cost= 0.16225211 training accuracy= 0.98762375 test accuracy = 0.8769231\n",
            "Iteration: 2200 cost= 0.15218912 training accuracy= 0.98762375 test accuracy = 0.87884617\n",
            "Iteration: 2300 cost= 0.14287625 training accuracy= 0.98844886 test accuracy = 0.8769231\n",
            "Iteration: 2400 cost= 0.13424745 training accuracy= 0.99174917 test accuracy = 0.875\n",
            "Iteration: 2500 cost= 0.12624235 training accuracy= 0.9925743 test accuracy = 0.875\n",
            "Iteration: 2600 cost= 0.11880574 training accuracy= 0.9933993 test accuracy = 0.8769231\n",
            "Iteration: 2700 cost= 0.11188737 training accuracy= 0.9942244 test accuracy = 0.8769231\n",
            "Iteration: 2800 cost= 0.105358 training accuracy= 0.9942244 test accuracy = 0.8769231\n",
            "Iteration: 2900 cost= 0.09925132 training accuracy= 0.9942244 test accuracy = 0.8769231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdTueAVyN8CL",
        "colab_type": "text"
      },
      "source": [
        "With this simple implementation, we have 99% training accuracy and 87% test accuracy. This shows the prevalance of large variance which we will try to reduce with some regularizers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H7boEyvs6E-",
        "colab_type": "text"
      },
      "source": [
        "## **Regularizations:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTpCzGIFs-x3",
        "colab_type": "text"
      },
      "source": [
        "### **Dropout:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t4ucgL0OLT1",
        "colab_type": "text"
      },
      "source": [
        "Like in the previous project, here we perform dropout regularization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbBcbSs_bO0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagation_dropout(A, parameters, keep_prob):\n",
        "  L = len(layers_dims)\n",
        "  for i in range(L-2):\n",
        "    W = parameters['W' + str(i+1)]\n",
        "    b = parameters['b' + str(i+1)]\n",
        "    A = tf.nn.dropout(A, rate = 1-keep_prob[i])\n",
        "    A = tf.nn.relu(tf.matmul(W, A) + b)\n",
        "\n",
        "  WL = parameters['W' + str(L-1)]\n",
        "  bL = parameters['b' + str(L-1)]\n",
        "  A = tf.nn.dropout(A, rate = 1-keep_prob[L-1])\n",
        "  ZL = tf.matmul(WL, A) + bL\n",
        "  return ZL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY1yzOnBuMx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_dropout(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001):\n",
        "  ops.reset_default_graph()\n",
        "  X = tf.placeholder(\"float\", shape = (X_train_flat.shape[0], None), name = 'X')\n",
        "  y = tf.placeholder(\"float\", shape = [1, None], name = 'y')\n",
        "  keep_prob = tf.placeholder(\"float\", shape = [3], name = 'keep_prob')\n",
        "  parameters = initialize_deep()\n",
        "  ZL = forward_propagation_dropout(X, parameters, keep_prob)\n",
        "  cost = cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = ZL, labels = y))\n",
        "  activation = tf.math.sigmoid(ZL)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "  prediction = tf.round(activation)\n",
        "  correct = tf.cast(tf.equal(prediction, y), dtype=tf.float32)\n",
        "  accuracy = tf.reduce_mean(correct)\n",
        "\n",
        "  init = tf.global_variables_initializer()\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    x_entropy = []\n",
        "    for i in range(4000):\n",
        "      avg_cost = 0\n",
        "      _, avg_cost = sess.run([optimizer, cost], feed_dict = {X: X_train_flat, y:y_train_flat, keep_prob:[.5, .5, 1]})\n",
        "      x_entropy.append(avg_cost)\n",
        "      temp_train_acc = sess.run(accuracy, feed_dict={X: X_train_flat, y: y_train_flat, keep_prob:[.5, .5, 1]})\n",
        "      temp_test_acc = sess.run(accuracy, feed_dict={X: X_test_flat, y: y_test_flat, keep_prob:[1, 1, 1]})\n",
        "      if i % 100 == 0:\n",
        "        print(\"Iteration:\", i, \"cost=\", avg_cost, \"training accuracy=\", temp_train_acc, \"test accuracy =\", temp_test_acc)\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfcEqBLcyN56",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "b2c68e33-67af-4e15-864a-caebcb55a43d"
      },
      "source": [
        "parameters = model_dropout(X_train_flat, y_train_flat, X_test_flat, y_test_flat, learning_rate = 0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0 cost= 0.6998343 training accuracy= 0.6460396 test accuracy = 0.7019231\n",
            "Iteration: 100 cost= 0.13782416 training accuracy= 0.9249175 test accuracy = 0.86923075\n",
            "Iteration: 200 cost= 0.13731122 training accuracy= 0.9414191 test accuracy = 0.87884617\n",
            "Iteration: 300 cost= 0.14313164 training accuracy= 0.929868 test accuracy = 0.875\n",
            "Iteration: 400 cost= 0.15237069 training accuracy= 0.9480198 test accuracy = 0.8730769\n",
            "Iteration: 500 cost= 0.121123195 training accuracy= 0.9430693 test accuracy = 0.88653845\n",
            "Iteration: 600 cost= 0.1218443 training accuracy= 0.94966996 test accuracy = 0.88461536\n",
            "Iteration: 700 cost= 0.12154193 training accuracy= 0.9480198 test accuracy = 0.88461536\n",
            "Iteration: 800 cost= 0.09585942 training accuracy= 0.94966996 test accuracy = 0.88076925\n",
            "Iteration: 900 cost= 0.10624713 training accuracy= 0.9447195 test accuracy = 0.88269234\n",
            "Iteration: 1000 cost= 0.116999745 training accuracy= 0.9537954 test accuracy = 0.87884617\n",
            "Iteration: 1100 cost= 0.10973575 training accuracy= 0.95627064 test accuracy = 0.88269234\n",
            "Iteration: 1200 cost= 0.12729919 training accuracy= 0.9447195 test accuracy = 0.88461536\n",
            "Iteration: 1300 cost= 0.12872815 training accuracy= 0.9620462 test accuracy = 0.87884617\n",
            "Iteration: 1400 cost= 0.10368181 training accuracy= 0.94966996 test accuracy = 0.8923077\n",
            "Iteration: 1500 cost= 0.1133967 training accuracy= 0.94224423 test accuracy = 0.88076925\n",
            "Iteration: 1600 cost= 0.09489386 training accuracy= 0.93234324 test accuracy = 0.88269234\n",
            "Iteration: 1700 cost= 0.10678375 training accuracy= 0.9480198 test accuracy = 0.88653845\n",
            "Iteration: 1800 cost= 0.096611954 training accuracy= 0.94966996 test accuracy = 0.88269234\n",
            "Iteration: 1900 cost= 0.1078024 training accuracy= 0.94636965 test accuracy = 0.87884617\n",
            "Iteration: 2000 cost= 0.122124955 training accuracy= 0.95627064 test accuracy = 0.8903846\n",
            "Iteration: 2100 cost= 0.10576307 training accuracy= 0.9488449 test accuracy = 0.88846153\n",
            "Iteration: 2200 cost= 0.11695249 training accuracy= 0.9471947 test accuracy = 0.89615387\n",
            "Iteration: 2300 cost= 0.107130155 training accuracy= 0.9430693 test accuracy = 0.88076925\n",
            "Iteration: 2400 cost= 0.11528723 training accuracy= 0.9405941 test accuracy = 0.88461536\n",
            "Iteration: 2500 cost= 0.100853376 training accuracy= 0.94224423 test accuracy = 0.8903846\n",
            "Iteration: 2600 cost= 0.11367652 training accuracy= 0.95627064 test accuracy = 0.88269234\n",
            "Iteration: 2700 cost= 0.11224957 training accuracy= 0.9414191 test accuracy = 0.88846153\n",
            "Iteration: 2800 cost= 0.10843684 training accuracy= 0.9488449 test accuracy = 0.88846153\n",
            "Iteration: 2900 cost= 0.09644381 training accuracy= 0.9405941 test accuracy = 0.8903846\n",
            "Iteration: 3000 cost= 0.10420282 training accuracy= 0.9554455 test accuracy = 0.8903846\n",
            "Iteration: 3100 cost= 0.111303106 training accuracy= 0.94966996 test accuracy = 0.88461536\n",
            "Iteration: 3200 cost= 0.110678345 training accuracy= 0.95049506 test accuracy = 0.8923077\n",
            "Iteration: 3300 cost= 0.1075953 training accuracy= 0.9471947 test accuracy = 0.8903846\n",
            "Iteration: 3400 cost= 0.11311193 training accuracy= 0.93976897 test accuracy = 0.8942308\n",
            "Iteration: 3500 cost= 0.10173119 training accuracy= 0.9488449 test accuracy = 0.8923077\n",
            "Iteration: 3600 cost= 0.11280303 training accuracy= 0.9628713 test accuracy = 0.89615387\n",
            "Iteration: 3700 cost= 0.10896265 training accuracy= 0.9513201 test accuracy = 0.88076925\n",
            "Iteration: 3800 cost= 0.10590032 training accuracy= 0.9570957 test accuracy = 0.8903846\n",
            "Iteration: 3900 cost= 0.09376196 training accuracy= 0.94966996 test accuracy = 0.88846153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Opgz-lj1OQ-O",
        "colab_type": "text"
      },
      "source": [
        "After dropout, the gap between training and test accuracy has decreased. The This is a good result as it means that the overfitting problem is reduced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0yfK6urIAgN",
        "colab_type": "text"
      },
      "source": [
        "### L2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzqn3vE-OcbR",
        "colab_type": "text"
      },
      "source": [
        "After dropout, not let us implement the L2 regularization. The main change here is when we compute the cost, as now in the cost function, we also need to minimize the sum of Frobenius norm of the weight matrices for each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yKpI8kyyR1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beta = 0.01\n",
        "def model_L2(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001):\n",
        "  ops.reset_default_graph()\n",
        "  X = tf.placeholder(\"float\", shape = (X_train_flat.shape[0], None), name = 'X')\n",
        "  y = tf.placeholder(\"float\", shape = [1, None], name = 'y')\n",
        "  parameters = initialize_deep()\n",
        "  ZL = forward_propagation(X, parameters)\n",
        "  cost = cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = ZL, labels = y))\n",
        "  regularizer = 0\n",
        "  for i in range(len(layers_dims)-2):\n",
        "    regularizer+= tf.nn.l2_loss(parameters['W'+str(i+1)])\n",
        "  cost = tf.reduce_mean(cost + beta * regularizer)\n",
        "\n",
        "  activation = tf.math.sigmoid(ZL)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "  prediction = tf.round(activation)\n",
        "  correct = tf.cast(tf.equal(prediction, y), dtype=tf.float32)\n",
        "  accuracy = tf.reduce_mean(correct)\n",
        "\n",
        "  init = tf.global_variables_initializer()\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    x_entropy = []\n",
        "    for i in range(3000):\n",
        "      avg_cost = 0\n",
        "      _, avg_cost = sess.run([optimizer, cost], feed_dict = {X: X_train_flat, y:y_train_flat})\n",
        "      x_entropy.append(avg_cost)\n",
        "      temp_train_acc = sess.run(accuracy, feed_dict={X: X_train_flat, y: y_train_flat})\n",
        "      temp_test_acc = sess.run(accuracy, feed_dict={X: X_test_flat, y: y_test_flat})\n",
        "      if i % 100 == 0:\n",
        "        print(\"Iteration:\", i, \"cost=\", avg_cost, \"training accuracy=\", temp_train_acc, \"test accuracy =\", temp_test_acc)\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDd6nTf9IkiG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "169af7c4-82db-473c-cfac-c39d01551759"
      },
      "source": [
        "parameters = model_L2(X_train_flat, y_train_flat, X_test_flat, y_test_flat, learning_rate = 0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0 cost= 0.74208575 training accuracy= 0.7714521 test accuracy = 0.72692305\n",
            "Iteration: 100 cost= 0.24779354 training accuracy= 0.9579208 test accuracy = 0.8923077\n",
            "Iteration: 200 cost= 0.19267541 training accuracy= 0.9818482 test accuracy = 0.9096154\n",
            "Iteration: 300 cost= 0.15883157 training accuracy= 0.9925743 test accuracy = 0.9096154\n",
            "Iteration: 400 cost= 0.13607498 training accuracy= 0.9950495 test accuracy = 0.9096154\n",
            "Iteration: 500 cost= 0.11985906 training accuracy= 0.99834985 test accuracy = 0.9076923\n",
            "Iteration: 600 cost= 0.10725707 training accuracy= 1.0 test accuracy = 0.9096154\n",
            "Iteration: 700 cost= 0.09743647 training accuracy= 1.0 test accuracy = 0.9096154\n",
            "Iteration: 800 cost= 0.08991629 training accuracy= 1.0 test accuracy = 0.91346157\n",
            "Iteration: 900 cost= 0.08284359 training accuracy= 1.0 test accuracy = 0.9115385\n",
            "Iteration: 1000 cost= 0.077466115 training accuracy= 1.0 test accuracy = 0.9076923\n",
            "Iteration: 1100 cost= 0.07285159 training accuracy= 1.0 test accuracy = 0.91346157\n",
            "Iteration: 1200 cost= 0.06848871 training accuracy= 1.0 test accuracy = 0.91346157\n",
            "Iteration: 1300 cost= 0.06457499 training accuracy= 1.0 test accuracy = 0.91346157\n",
            "Iteration: 1400 cost= 0.061245576 training accuracy= 1.0 test accuracy = 0.9153846\n",
            "Iteration: 1500 cost= 0.06062781 training accuracy= 1.0 test accuracy = 0.90192306\n",
            "Iteration: 1600 cost= 0.055631723 training accuracy= 1.0 test accuracy = 0.9153846\n",
            "Iteration: 1700 cost= 0.053194225 training accuracy= 1.0 test accuracy = 0.91346157\n",
            "Iteration: 1800 cost= 0.05098657 training accuracy= 1.0 test accuracy = 0.9153846\n",
            "Iteration: 1900 cost= 0.04962363 training accuracy= 1.0 test accuracy = 0.90384614\n",
            "Iteration: 2000 cost= 0.047044367 training accuracy= 1.0 test accuracy = 0.9096154\n",
            "Iteration: 2100 cost= 0.04544194 training accuracy= 1.0 test accuracy = 0.9076923\n",
            "Iteration: 2200 cost= 0.05352946 training accuracy= 1.0 test accuracy = 0.88653845\n",
            "Iteration: 2300 cost= 0.04220516 training accuracy= 1.0 test accuracy = 0.9076923\n",
            "Iteration: 2400 cost= 0.04078254 training accuracy= 1.0 test accuracy = 0.9076923\n",
            "Iteration: 2500 cost= 0.039446577 training accuracy= 1.0 test accuracy = 0.9076923\n",
            "Iteration: 2600 cost= 0.03818334 training accuracy= 1.0 test accuracy = 0.9076923\n",
            "Iteration: 2700 cost= 0.036988176 training accuracy= 1.0 test accuracy = 0.9076923\n",
            "Iteration: 2800 cost= 0.035858907 training accuracy= 1.0 test accuracy = 0.9076923\n",
            "Iteration: 2900 cost= 0.03518957 training accuracy= 1.0 test accuracy = 0.9096154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz3_Zl6vOrXz",
        "colab_type": "text"
      },
      "source": [
        "This reguaization has increased the test accuracy to 90%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGXc6jN6KHjR",
        "colab_type": "text"
      },
      "source": [
        "### **L2 + Dropout:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tWPJbbpOvWK",
        "colab_type": "text"
      },
      "source": [
        "Now finally, let us add the two regulaizations together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qlj6LG0IoCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_regularization(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001):\n",
        "  ops.reset_default_graph()\n",
        "  X = tf.placeholder(\"float\", shape = (X_train_flat.shape[0], None), name = 'X')\n",
        "  y = tf.placeholder(\"float\", shape = [1, None], name = 'y')\n",
        "  keep_prob = tf.placeholder(\"float\", shape = [3], name = 'keep_prob')\n",
        "  parameters = initialize_deep()\n",
        "  ZL = forward_propagation_dropout(X, parameters, keep_prob)\n",
        "  cost = cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = ZL, labels = y))\n",
        "  regularizer = 0\n",
        "  for i in range(len(layers_dims)-2):\n",
        "    regularizer+= tf.nn.l2_loss(parameters['W'+str(i+1)])\n",
        "  cost = tf.reduce_mean(cost + beta * regularizer)\n",
        "\n",
        "  activation = tf.math.sigmoid(ZL)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "  prediction = tf.round(activation)\n",
        "  correct = tf.cast(tf.equal(prediction, y), dtype=tf.float32)\n",
        "  accuracy = tf.reduce_mean(correct)\n",
        "\n",
        "  init = tf.global_variables_initializer()\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    x_entropy = []\n",
        "    for i in range(4000):\n",
        "      avg_cost = 0\n",
        "      _, avg_cost = sess.run([optimizer, cost], feed_dict = {X: X_train_flat, y:y_train_flat, keep_prob:[.5, .5, 1]})\n",
        "      x_entropy.append(avg_cost)\n",
        "      temp_train_acc = sess.run(accuracy, feed_dict={X: X_train_flat, y: y_train_flat, keep_prob:[.5, .5, 1]})\n",
        "      temp_test_acc = sess.run(accuracy, feed_dict={X: X_test_flat, y: y_test_flat, keep_prob:[1, 1, 1]})\n",
        "      if i % 100 == 0:\n",
        "        print(\"Iteration:\", i, \"cost=\", avg_cost, \"training accuracy=\", temp_train_acc, \"test accuracy =\", temp_test_acc)\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDDsh-BHKfEk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "d08e492a-ea95-4f43-b12b-8c9b11e9c071"
      },
      "source": [
        "parameters = model_regularization(X_train_flat, y_train_flat, X_test_flat, y_test_flat, learning_rate = 0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0 cost= 0.749731 training accuracy= 0.57838285 test accuracy = 0.6403846\n",
            "Iteration: 100 cost= 0.42012957 training accuracy= 0.85231024 test accuracy = 0.8769231\n",
            "Iteration: 200 cost= 0.35636252 training accuracy= 0.8927393 test accuracy = 0.8769231\n",
            "Iteration: 300 cost= 0.348603 training accuracy= 0.9026403 test accuracy = 0.8903846\n",
            "Iteration: 400 cost= 0.3263884 training accuracy= 0.9158416 test accuracy = 0.875\n",
            "Iteration: 500 cost= 0.30300987 training accuracy= 0.900165 test accuracy = 0.8769231\n",
            "Iteration: 600 cost= 0.2968905 training accuracy= 0.9141914 test accuracy = 0.88461536\n",
            "Iteration: 700 cost= 0.2861185 training accuracy= 0.9092409 test accuracy = 0.88653845\n",
            "Iteration: 800 cost= 0.27241087 training accuracy= 0.9232673 test accuracy = 0.875\n",
            "Iteration: 900 cost= 0.2679056 training accuracy= 0.90841585 test accuracy = 0.875\n",
            "Iteration: 1000 cost= 0.26479322 training accuracy= 0.9240924 test accuracy = 0.88653845\n",
            "Iteration: 1100 cost= 0.24837644 training accuracy= 0.9240924 test accuracy = 0.8769231\n",
            "Iteration: 1200 cost= 0.26732472 training accuracy= 0.9265677 test accuracy = 0.8730769\n",
            "Iteration: 1300 cost= 0.24366572 training accuracy= 0.9232673 test accuracy = 0.86538464\n",
            "Iteration: 1400 cost= 0.2660147 training accuracy= 0.91831684 test accuracy = 0.88653845\n",
            "Iteration: 1500 cost= 0.23395649 training accuracy= 0.91831684 test accuracy = 0.86538464\n",
            "Iteration: 1600 cost= 0.2527949 training accuracy= 0.9133663 test accuracy = 0.8769231\n",
            "Iteration: 1700 cost= 0.24209516 training accuracy= 0.92161715 test accuracy = 0.88461536\n",
            "Iteration: 1800 cost= 0.24876054 training accuracy= 0.9290429 test accuracy = 0.86730766\n",
            "Iteration: 1900 cost= 0.22638386 training accuracy= 0.9191419 test accuracy = 0.88461536\n",
            "Iteration: 2000 cost= 0.247669 training accuracy= 0.919967 test accuracy = 0.88846153\n",
            "Iteration: 2100 cost= 0.23289622 training accuracy= 0.92574257 test accuracy = 0.875\n",
            "Iteration: 2200 cost= 0.23841634 training accuracy= 0.9249175 test accuracy = 0.87884617\n",
            "Iteration: 2300 cost= 0.22210367 training accuracy= 0.9372937 test accuracy = 0.89615387\n",
            "Iteration: 2400 cost= 0.24792811 training accuracy= 0.9240924 test accuracy = 0.875\n",
            "Iteration: 2500 cost= 0.23293342 training accuracy= 0.9265677 test accuracy = 0.88653845\n",
            "Iteration: 2600 cost= 0.23371208 training accuracy= 0.93151814 test accuracy = 0.88846153\n",
            "Iteration: 2700 cost= 0.2524755 training accuracy= 0.9240924 test accuracy = 0.86923075\n",
            "Iteration: 2800 cost= 0.21726091 training accuracy= 0.93564355 test accuracy = 0.88461536\n",
            "Iteration: 2900 cost= 0.23721233 training accuracy= 0.9249175 test accuracy = 0.8557692\n",
            "Iteration: 3000 cost= 0.22369437 training accuracy= 0.92161715 test accuracy = 0.8596154\n",
            "Iteration: 3100 cost= 0.22081977 training accuracy= 0.9282178 test accuracy = 0.875\n",
            "Iteration: 3200 cost= 0.22847661 training accuracy= 0.929868 test accuracy = 0.87884617\n",
            "Iteration: 3300 cost= 0.22649267 training accuracy= 0.929868 test accuracy = 0.86923075\n",
            "Iteration: 3400 cost= 0.21231711 training accuracy= 0.9141914 test accuracy = 0.875\n",
            "Iteration: 3500 cost= 0.21528246 training accuracy= 0.9207921 test accuracy = 0.9\n",
            "Iteration: 3600 cost= 0.22382192 training accuracy= 0.9092409 test accuracy = 0.86923075\n",
            "Iteration: 3700 cost= 0.21025908 training accuracy= 0.9331683 test accuracy = 0.86730766\n",
            "Iteration: 3800 cost= 0.20670629 training accuracy= 0.92244226 test accuracy = 0.88269234\n",
            "Iteration: 3900 cost= 0.23012449 training accuracy= 0.9306931 test accuracy = 0.8730769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoKaFLu8Oz5A",
        "colab_type": "text"
      },
      "source": [
        "With this implementation, the problem of overfitting has reduced significantly. In order to increase the bias in the data, we need to better model the features, maybe use natural langauge processing methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILr0FU-QPDdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}